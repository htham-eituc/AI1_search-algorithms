"""
Local Search Algorithms
========================
HillClimbing           — Steepest Ascent on a 2-D grid (inherits BaseGraphSearch)
HillClimbingContinuous — Steepest Ascent for continuous functions (inherits BaseMetaheuristic)

Both use random restarts to escape local optima.
"""

import time
import numpy as np
import sys
import os

sys.path.append(os.path.join(os.path.dirname(__file__), '..', '..'))
from algorithms.base import BaseGraphSearch, BaseMetaheuristic


# ── Shared heuristic ──────────────────────────────────────────────────────────

def _manhattan(node, goal) -> float:
    return abs(node[0] - goal[0]) + abs(node[1] - goal[1])


# ══════════════════════════════════════════════════════════════════════════════
#  1. Hill Climbing — Steepest Ascent (Grid / Discrete)
# ══════════════════════════════════════════════════════════════════════════════

class HillClimbing(BaseGraphSearch):
    """
    Hill Climbing (Steepest Ascent) on a 2-D grid.

    At each step, ALL valid neighbors are evaluated; the one with the
    lowest Manhattan distance to the goal is selected (steepest descent
    on the heuristic landscape). Random restarts escape dead ends.

    Not guaranteed to find the globally shortest path — susceptible to
    local optima (dead ends / narrow corridors).

    Parameters
    ----------
    max_restarts : int
        Number of random restarts when stuck in a dead end.
    seed : int or None
        Random seed for reproducibility.
    """

    def __init__(self, grid, start_node, end_node,
                 max_restarts: int = 10, seed: int = None):
        super().__init__("HillClimbing", grid, start_node, end_node)
        self.max_restarts = max_restarts
        self.seed         = seed

    def _free_cells(self):
        """All passable (non-wall) cell coordinates."""
        return list(zip(*np.where(self.grid == 0)))

    def solve(self):
        t0 = time.time()
        if self.seed is not None:
            np.random.seed(self.seed)

        start      = self.start_node
        goal       = self.end_node
        free_cells = self._free_cells()
        best_path  = None

        for restart in range(self.max_restarts + 1):
            current = start if restart == 0 else \
                      free_cells[np.random.randint(len(free_cells))]
            path    = [current]
            visited = {current}

            while current != goal:
                neighbors = [n for n in self.get_neighbors(current)
                             if n not in visited]
                if not neighbors:
                    break                       # dead end → restart

                self.nodes_expanded += 1
                self.explored_path.append(current)

                # Steepest ascent: pick neighbor minimizing h(n, goal)
                current = min(neighbors, key=lambda n: _manhattan(n, goal))
                path.append(current)
                visited.add(current)

            if current == goal:
                if best_path is None or len(path) < len(best_path):
                    best_path = path
                break

        if best_path is not None:
            self.best_solution = best_path
            self.best_fitness  = len(best_path) - 1
        else:
            self.best_solution = []
            self.best_fitness  = float('inf')

        self.execution_time = time.time() - t0
        return self


# ══════════════════════════════════════════════════════════════════════════════
#  2. Hill Climbing — Steepest Ascent (Continuous)
# ══════════════════════════════════════════════════════════════════════════════

class HillClimbingContinuous(BaseMetaheuristic):
    """
    Hill Climbing (Steepest Ascent) for continuous function minimization.

    At each step, `pop_size` neighbors are generated by Gaussian perturbation;
    the best improvement is accepted (steepest descent). Step-size decay and
    random restarts help escape shallow local optima.

    Inherits BaseMetaheuristic so results plug directly into the same
    visualization pipeline as TLBO / SFO / CA.

    Parameters
    ----------
    objective_func : callable
        f(population) -> np.ndarray of shape (n,). Must be vectorised.
    pop_size : int
        Number of neighbors sampled per iteration (controls search width).
    max_iter : int
    bounds : np.ndarray, shape (dim, 2)
    dim : int
    step_size : float
        Initial Gaussian perturbation std deviation.
    step_decay : float
        Multiplicative decay applied to step_size each iteration.
    max_restarts : int
        Restart from a random point after `patience` non-improving steps.
    patience : int
        Non-improving steps before triggering a random restart.
    seed : int or None
    """

    def __init__(self, objective_func, pop_size: int = 8, max_iter: int = 500,
                 bounds: np.ndarray = None, dim: int = None,
                 step_size: float = 0.5, step_decay: float = 0.995,
                 max_restarts: int = 10, patience: int = 30,
                 seed: int = None):
        super().__init__("HillClimbingContinuous", objective_func,
                         pop_size, max_iter, bounds, dim)
        self.step_size    = step_size
        self.step_decay   = step_decay
        self.max_restarts = max_restarts
        self.patience     = patience
        self.seed         = seed

    def solve(self):
        if self.seed is not None:
            np.random.seed(self.seed)

        lb   = self.bounds[:, 0]
        ub   = self.bounds[:, 1]
        t0   = time.time()
        step = self.step_size

        # ── Initialisation ────────────────────────────────────────────────────
        current     = np.random.uniform(lb, ub, size=self.dim)
        current_fit = self.objective_func(current.reshape(1, -1))[0]

        self.best_solution = current.copy()
        self.best_fitness  = current_fit

        restarts_done = 0
        no_improve    = 0

        for t in range(self.max_iter):
            step *= self.step_decay

            # ── Steepest ascent: sample pop_size neighbors ────────────────────
            noise      = np.random.normal(0, step, size=(self.pop_size, self.dim))
            neighbors  = np.clip(current + noise, lb, ub)
            nb_fitness = self.objective_func(neighbors)

            best_idx = np.argmin(nb_fitness)
            if nb_fitness[best_idx] < current_fit:
                current     = neighbors[best_idx].copy()
                current_fit = nb_fitness[best_idx]
                no_improve  = 0
            else:
                no_improve += 1

            if current_fit < self.best_fitness:
                self.best_fitness  = current_fit
                self.best_solution = current.copy()

            # ── Random restart when stuck ──────────────────────────────────────
            if no_improve >= self.patience and restarts_done < self.max_restarts:
                current     = np.random.uniform(lb, ub, size=self.dim)
                current_fit = self.objective_func(current.reshape(1, -1))[0]
                step        = self.step_size          # reset step on restart
                no_improve  = 0
                restarts_done += 1

            # ── Track metrics ─────────────────────────────────────────────────
            self.convergence_curve[t]     = self.best_fitness
            self.average_fitness_curve[t] = current_fit
            self.diversity_curve[t]       = step       # step size proxies diversity

        self.execution_time = time.time() - t0
        return self.get_results()


# ══════════════════════════════════════════════════════════════════════════════
#  Demo
# ══════════════════════════════════════════════════════════════════════════════

if __name__ == "__main__":

    # ── Grid Hill Climbing ────────────────────────────────────────────────────
    np.random.seed(5)
    ROWS, COLS = 15, 15
    grid = np.zeros((ROWS, COLS), dtype=int)
    for r in range(ROWS):
        for c in range(COLS):
            if (r, c) not in [(0, 0), (14, 14)] and np.random.random() < 0.25:
                grid[r, c] = 1

    START, GOAL = (0, 0), (14, 14)
    hc_grid = HillClimbing(grid, START, GOAL, max_restarts=10, seed=42)
    hc_grid.solve()
    r = hc_grid.get_results()
    found = r['best_fitness'] < float('inf')
    print(f"[HillClimbing Grid]")
    print(f"  Found={found} | path={r['path_length']} | "
          f"nodes={r['nodes_expanded']} | "
          f"time={r['execution_time_seconds']*1000:.2f}ms\n")

    # ── Continuous Hill Climbing ───────────────────────────────────────────────
    def sphere(pop: np.ndarray) -> np.ndarray:
        if pop.ndim == 1:
            pop = pop.reshape(1, -1)
        return np.sum(pop ** 2, axis=1)

    DIM    = 10
    BOUNDS = np.array([[-5.12, 5.12]] * DIM)

    hc_cont = HillClimbingContinuous(
        sphere, pop_size=8, max_iter=500,
        bounds=BOUNDS, dim=DIM,
        step_size=0.5, max_restarts=10, seed=42
    )
    hc_cont.solve()
    r2 = hc_cont.get_results()
    print(f"[HillClimbingContinuous] Sphere dim={DIM}")
    print(f"  Best fitness  : {r2['best_fitness']:.6e}")
    print(f"  Execution time: {r2['execution_time_seconds']*1000:.2f}ms")
